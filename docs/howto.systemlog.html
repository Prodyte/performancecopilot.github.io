<!DOCTYPE html>
<html>
<head>
<meta charset='utf-8'>
<meta content='IE=edge,chrome=1' http-equiv='X-UA-Compatible'>
<meta content='initial-scale=1' name='viewport'>
<title>Performance Co-Pilot</title>
<link href='/assets/css/screen.css' media='all' rel='stylesheet' type='text/css'>
<link href='/assets/css/master.css' media='all' rel='stylesheet' type='text/css'>
<link href='/images/pcp.ico' rel='shortcut icon' type='image/ico'>

</head>
<body>
<header class='global-header'>
<div class='row'>
<h1 class='global-logo colspan12-6 colspan8-4 colspan6-3 colspan2-1 as-grid'>
<a href='/index.html'>Performance Co-Pilot</a>
</h1>
<nav class='global-header__navigation colspan12-6 colspan8-6 colspan6-4 colspan2-1 as-grid'>
<ul>
<li>
<a href='/features.html'>Features</a>
</li>
<li>
<a href='/documentation.html'>Documentation</a>
</li>
<li>
<a href='/download.html'>Download</a>
</li>
<li>
<a href='/community.html'>Get Involved</a>
</li>
</ul>
</nav>
</div>
</header>

<div class='site-content'>
<div class='how-to is-typeset'>
<div class='row-parent'>
<div class='row'>
<div class='docpage'>
﻿
 <h1 align="CENTER" style="margin-top: 0.48cm; margin-bottom: 0.32cm">
  <font size="7">
   System Event Log Instrumentation
  </font>
 </h1>
 <table align="RIGHT" border="0" cellpadding="5" cellspacing="10" width="15%">
  <tr>
   <td>
    <pre><img alt="" border="0" height="16" src="images/system-search.png" width="16"/>  <i>Tools</i><br/>
pmval
pmevent
pmchart
pmdalogger
pmdarsyslog
pmdaelasticsearch
</pre>
   </td>
  </tr>
 </table>
 <p>
  Capturing system event log information from a collection of geographically
distributed systems in a reliable and useful way presents many challenges.
In order for operations and engineering triage teams to be able to filter
important events from this huge amount of information, events need to be
shipped to a central location, reliably, where they can be indexed and
searched.
 </p>
 <p>
  Ideally this is done in a way that is both timely and removed
from the systems under observation.  Being timely allows events relevant
to an active production problem are available to triage personnel.  Being
removed from the production system allows for a reduction of impact to the
observed system, and also allows for collation of events from cooperating
systems (separate databases, application servers, web servers and storage
servers, for example).
 </p>
 <p>
  In addition to the events logged by the operating system kernel and the
system daemons, it is also highly desirable to capture application events
as well.  For minimal operational maintenance overhead, these should all
be managed by a single, reliable event shipping system for application
logs.
This case study documents the design and deployment of one such system,
and focuses on the performance instrumentation used for monitoring and
problem diagnosis in the event management service itself.
 </p>
 <p>
  <br/>
 </p>
 <table border="0" cellpadding="0" cellspacing="0" width="100%">
  <tr>
   <td width="100%">
    <p align="LEFT">
     <font size="5">
      <b>
       Technology Choices
      </b>
     </font>
    </p>
   </td>
  </tr>
 </table>
 <p>
  A brief overview of the technologies used, and why they
were selected from the many alternatives available, follows.
As this was likely to become a critical system infrastructure component,
and the organisation had existing operational and engineering expertise
in Linux and Java, open source solutions were generally preferred.
The opportunity to evaluate new technologies was foreseen and affected
some of the choices made.
 </p>
 <p>
  <b>
   Event capturing and transport
  </b>
 </p>
 <p>
  <a href="http://www.rsyslog.com/">
   rsyslog
  </a>
  is the default system log daemon on the Linux distribution used, and
it provides efficient, reliable end-to-end delivery.
It turned out to be easily instrumented - providing both its own metrics
and mechanisms for extending with metrics specific to our own needs.
 </p>
 <p>
  <b>
   Index and search functionality
  </b>
 </p>
 <p>
  <a href="http://www.elasticsearch.org/">
   elasticsearch
  </a>
  and
  <a href="http://mobz.github.com/elasticsearch-head/">
   elasticsearch-head
  </a>
  are used as the mechanisms for indexing event messages as they arrive, and
providing event search functionality.
 </p>
 <p>
  <br/>
 </p>
 <table border="0" cellpadding="0" cellspacing="0" width="100%">
  <tr>
   <td width="100%">
    <p align="LEFT">
     <font size="5">
      <b>
       High-level Design
      </b>
     </font>
    </p>
   </td>
  </tr>
 </table>
 <p>
  The design caters for a fairly typical medium-sized web application
deployment.
Each data centre hosting the application (distributed throughout the
world) contains several racks of equipment dedicated to delivering the
service.
Individual racks are populated with closely cooperating machines, each
generating their own system and application logs.
 </p>
 <p>
 </p>
 <center>
  <img align="MIDDLE" alt="" height="370" src="images/systemlogs.png" width="576"/>
 </center>
 <p>
  Initially, all logs are streamed directly to a central machine in the
local deployment (typically, within the same rack).
On those machines where application logs are being generated the logs
are immediately merged into the system log stream using the syslog
networking protocol.
We made use of the syslog "facility" concept, to designate all
application logs to be "local0" facility.
 </p>
 <p>
  On average, one instance would typically generate several gigabytes of
application logs each day.
The system log traffic is a tiny fraction of that.
 </p>
 <p>
  This separation of application log messages from system log messages
became most useful at the final end point (indexing), since the facility
identifier travels with each message (as does source hostname, timestamp
and priority information).
Since elasticsearch was chosen as the indexing and search technology,
the application was modified (via Log4J configuration) to generate logs
messages in
  <a href="http://json.org">
   JSON
  </a>
  format.
This allowed the information-rich application logs to maintain a clear
separation of critical information (request ID, user ID, identifiers for
the pages being accessed, and many more application level details), such
that these fields could be separately indexed on the receiving end.
 </p>
 <p>
  For the remainder of the operating system and service level messages,
conversion to JSON was performed at the final stage, just prior to being
indexed, with the syslog message being one indexed field.  The source host
and event generation timestamp that travelled with the message are also
separately indexed.
 </p>
 <p>
  <br/>
 </p>
 <table border="0" cellpadding="0" cellspacing="0" width="100%">
  <tr>
   <td width="100%">
    <p align="LEFT">
     <font size="5">
      <b>
       Deployment and Instrumentation: rsyslog
      </b>
     </font>
    </p>
   </td>
  </tr>
 </table>
 <p>
  It was highly desirable to gain insight into many levels of the event
transfer process.
Identifying hosts generating too much traffic, or hosts not generating
any log traffic (misconfiguration) was initially important - so, event
counters, and cumulative byte counts of events generated was required.
It was also important to be able to see these data rates alongside the
network interface data rates, to understand the additional load generated
through live streaming of the event log data.
 </p>
 <p>
  <b>
   Instrumenting
   <i>
    rsyslog
   </i>
   internals
  </b>
 </p>
 <p>
  The
  <i>
   rsyslog
  </i>
  daemon runs on each and every machine involved, so
low overhead is a desirable attribute in any instrumentation added.
Not all of the machines are configured the same though (the event
"forwarders" and event "indexers" differ to the
"leaf" nodes, which form most of the event sources), and
so the configuration of
  <i>
   rsyslog
  </i>
  required some care.
 </p>
 <p>
  We took a two-pronged approach to instrumenting the
  <i>
   rsyslog
  </i>
  processes.  On inspection, it turned out that there is some existing
instrumentation on the
  <i>
   rsyslog
  </i>
  internal workings available.
It must be explicitly enabled - both at the source level and at runtime.
To enable the instrumentation in an
  <i>
   rsyslog
  </i>
  build, the
  <tt>
   --enable-impstats
  </tt>
  build configuration flag is needed.
Then, enabling the statistics and exporting them at runtime requires
the following additions to the
  <i>
   rsyslog
  </i>
  configuration:
 </p>
 <pre>
	# Provide rsyslog statistics
	$ModLoad impstats
	$PStatsInterval 5
</pre>
 <p>
  This instructs
  <i>
   rsyslog
  </i>
  to load the statistics module, and to
export the current state of its statistics on a 5 second interval.
We can then capture those statistics to an arbitrary output channel,
again using a configuration addition.
 </p>
 <table border="0" cellpadding="0" cellspacing="0" width="100%">
  <tr>
   <td>
    <img align="RIGHT" alt="" border="0" height="506" src="images/systemlog-arrival.png" width="398"/>
   </td>
   <td>
    <table border="0" cellpadding="0" cellspacing="15">
     <tr>
      <td>
       <p>
        We would like to incorporate the
        <i>
         rsyslog
        </i>
        statistics
  (giving insight into arrival rates and queueing behaviour) into our
  overall performance management strategy - tracking them alongside all 
  of our other metrics for comparison, graphing, alarming, etc.  This is
  acheived by a PMDA -
        <i>
         pmdarsyslog
        </i>
        - which exports these metrics:
       </p>
       <pre><i>
	rsyslog.interval
	rsyslog.queues.size
	rsyslog.queues.maxsize
	rsyslog.queues.full
	rsyslog.queues.enqueued
	rsyslog.imuxsock.submitted
	rsyslog.imuxsock.discarded
	rsyslog.imuxsock.numratelimiters
	rsyslog.elasticsearch.connfail
	rsyslog.elasticsearch.submits
	rsyslog.elasticsearch.success
	rsyslog.elasticsearch.failed </i></pre>
      </td>
     </tr>
     <tr>
      <td>
       <table border="0" cellpadding="10" cellspacing="20" width="100%">
        <tr>
         <td width="50%">
          <br/>
          Install
          <i>
           pmdarsyslog
          </i>
          :
          <pre><b># source /etc/pcp.env
# cd $PCP_PMDAS_DIR/rsyslog
# ./Install </b></pre>
         </td>
        </tr>
       </table>
      </td>
     </tr>
    </table>
   </td>
  </tr>
 </table>
 <p>
  The
  <i>
   rsyslog
  </i>
  configuration addition to achieve the link up
between
  <i>
   rsyslog
  </i>
  and
  <i>
   pmdarsyslog
  </i>
  is:
 </p>
 <pre>
	# Performance instrumentation
	syslog.info                     |/var/log/pcp/rsyslog/stats
</pre>
 <p>
  Note that once enabling this, care must be taken to ensure those stats
values are logged only where we wish.  In our case, we did not wish these
to end up in the system log file, nor being included in the set of values
being shipped for indexing.
The configuration entries that achieve this restriction in our scenario
are as follows:
 </p>
 <pre>
	*.*;local0.none;syslog.!=info   /var/log/messages
	*.*;syslog.!=info               @@log1;RSYSLOG_ForwardFormat
	$ActionExecOnlyWhenPreviousIsSuspended on
	&amp; @@log2
	&amp; /var/spool/rsyslog-buffer
	$ActionExecOnlyWhenPreviousIsSuspended off
</pre>
 <p>
  The
  <i>
   syslog.info
  </i>
  tag identifies the input channel where the
periodic
  <i>
   rsyslog
  </i>
  metric values are written.
So, the first line above prevents the
  <i>
   rsyslog
  </i>
  metric values
(
  <i>
   syslog.!=info
  </i>
  ) from appearing in the local system log file
(
  <i>
   /var/log/messages
  </i>
  ).
In addition, we prevent all local application log messages from going
there ("local0"), which is simply so we don't duplicate work that
the application is already doing itself.
The remaining configuration lines are those responsible for forwarding
local host messages to the central log forwarding server for a rack
(log1), and for failing over to a second log server (log2).
Note how we choose not to forward the
  <i>
   syslog.info
  </i>
  message class.
 </p>
 <p>
  From our findings so far, it is worth keep an eye (
  <i>
   pmie
  </i>
  ) on the
queue fullness and discard counts, as they appear to be handy indicators
that a node might not be be keeping up.
In addition, the interval metric should also be kept under surveilance -
if messages arrive more frequently than the configured interval (five
seconds in our case), then it indicates a node somewhere is forwarding
its instrumentation where it should not be (i.e. a misconfiguration).
 </p>
 <p>
  <b>
   Instrumenting
   <i>
    rsyslog
   </i>
   output channels
  </b>
 </p>
 <p>
  We can now extend the high level system log counters by drilling
down to specific output channels.
We use the ability of
  <i>
   rsyslog
  </i>
  to write to a named pipe once more -
this provides a coordination point between the PCP PMDA and the daemon,
without having to log to disk and deal with issues like filesystem free
space management, log rotation, and so forth.
 </p>
 <p>
  We extend our configuration with two more lines, as follows:
 </p>
 <pre>
	# Performance instrumentation
	syslog.info                     |/var/log/pcp/rsyslog/stats
	local0.*                        |/var/log/pcp/logger/applog
	*.*;local0.none;syslog.!=info   |/var/log/pcp/logger/syslog
</pre>
 <p>
  The final two lines are feeding all application and other traffic
into two named pipes (fifos) for processing.  These need to exist
before
  <i>
   rsyslog
  </i>
  starts up, and
  <i>
   pmdalogger
  </i>
  needs to be
configured (at PMDA install time) to be listening for data on these
files.
 </p>
 <table border="0" cellpadding="0" cellspacing="0" width="100%">
  <tr>
   <td>
    <table border="0" cellpadding="0" cellspacing="15">
     <tr>
      <td>
       <p>
        Once that is done, new metrics are then available that give
  insight into the number of events being seen for each class of log,
  as well as the total event traffic throughput for each:
       </p>
       <pre><i>
	logger.perfile.applog.bytes
	logger.perfile.applog.count
	logger.perfile.syslog.bytes
	logger.perfile.syslog.count </i></pre>
      </td>
     </tr>
     <tr>
      <td>
       <table border="0" cellpadding="10" cellspacing="20" width="100%">
        <tr>
         <td width="50%">
          <br/>
          Install
          <i>
           pmdalogger
          </i>
          :
          <pre><b># source /etc/pcp.env
# cd $PCP_PMDAS_DIR/logger
# ./Install </b></pre>
          <p>
           Follow the prompts and configure two logfile entries for the two
       named pipes.
          </p>
          <br/>
         </td>
        </tr>
       </table>
      </td>
     </tr>
    </table>
   </td>
   <td>
    <img align="RIGHT" alt="" border="0" src="images/systemlog-throughput.png"/>
   </td>
  </tr>
 </table>
 <p>
  <b>
   Deeper event analysis
  </b>
 </p>
 <p>
  Finally, this was an opportunity to begin to evaluate the utility of
recently introduced event tracing functionality in PCP.
The logger PMDA which was separately counting application and system log
traffic also had visibility to all event traffic, and has been used to
inspect log contents (for checking event arrival, and for verifying JSON
validity, etc) in what proved a much more convenient way than snooping
the raw network traffic.
 </p>
 <pre><i>
		logger.perfile.applog.records
		logger.perfile.syslog.records
</i></pre>
 <p>
  This was done with the command line utility
  <i>
   pmevent
  </i>
  , which is the
first and most simple of event tracing tools - but even in this basic
"text dump" form, insight was gained by being able to see the
exact traffic passing through each output stream, and some configuration
problems were diagnosed and resolved through its use.
 </p>
 <p>
 </p>
 <center>
  <img align="MIDDLE" alt="" height="455" src="images/systemlog-events.png" width="826"/>
 </center>
 <p>
  <br/>
 </p>
 <table border="0" cellpadding="0" cellspacing="0" width="100%">
  <tr>
   <td width="100%">
    <p align="LEFT">
     <font size="5">
      <b>
       A Quick Aside: Buffer Sizes
      </b>
     </font>
    </p>
   </td>
  </tr>
  <tr>
   <td width="100%">
    <br/>
    <p>
     If there is a chance of sending extremely large messages through
     <i>
      rsyslog
     </i>
     in your environment, such as JVM stack traces or the
like, we found it necessary to increase the default maximum message
size permitted:
    </p>
    <pre>
	# Global directives
	$MaxMessageSize 65536
	$PreserveFQDN on
</pre>
    <p>
     We also insisted that sending hosts fully qualified domain names are
always used, as some of the (global) hosts would otherwise have ambiguous
hostnames.
    </p>
    <p>
     Buffer sizes can also become problematic in the named pipe implementation
in the Linux kernel.  The default (and until relatively recently, unchangeable)
buffer size is 16 pages, which on the typical 4KiB page size system, gives us
only 64KiB.
Since late 2010, this can now be queried and modified though an fcntl(2),
and system wide limited are imposed (1MiB maximum, for non-root users).
    </p>
    <pre>
	cat /proc/sys/fs/pipe-max-size
	1048576
</pre>
    <p>
     If you're interested in the details of the pipe implementation, refer
to
     <tt>
      fs/pipe.c
     </tt>
     in the Linux kernel source.
    </p>
    <br/>
   </td>
  </tr>
 </table>
 <p>
  <br/>
 </p>
 <table border="0" cellpadding="0" cellspacing="0" width="100%">
  <tr>
   <td width="100%">
    <p align="LEFT">
     <font size="5">
      <b>
       Deployment and Instrumentation: elasticsearch
      </b>
     </font>
    </p>
   </td>
  </tr>
 </table>
 <p>
  <b>
   Instrumenting elasticsearch
  </b>
 </p>
 <p>
  We come to the final stage of our deployment.
System log messages are now flowing to our central repository at an
alarming rate, now we need an efficient way to index them immediately
as they arrive, and also to allow interactive queries of that indexed
data.
This turns out to have been made easy by the
  <i>
   elasticsearch
  </i>
  project, which is a distributed, RESTful, search engine built on top
of Lucene.
 </p>
 <p>
  A basic setup involves downloading the code, untarring, and running
  <i>
   bin/elasticsearch -f
  </i>
  .  That's it, with no more configuration than
that we have a working solution which correctly identifies the timestamp
fields as distinct from the other text.
 </p>
 <p>
  From there we can increase reliability and scalability in a number of
ways with
  <i>
   elasticsearch
  </i>
  , but that setup will differ for different
deployments and is outside the scope here.
 </p>
 <p>
  Configuring
  <i>
   rsyslog
  </i>
  to send messages in the JSON format which
the
  <i>
   elasticsearch
  </i>
  REST API uses, is a matter of configuring the
appropriate output module.
As was the case with the
  <i>
   rsyslog
  </i>
  stats module, this must be
explicitly enabled - both at the source level and at runtime.
To enable this functionality in an
  <i>
   rsyslog
  </i>
  build, the
  <tt>
   --enable-elasticsearch
  </tt>
  build configuration flag is needed.
 </p>
 <pre>
	#
	# Create a searchable index using elasticsearch
	# 
	$ModLoad omelasticsearch
	*.=info;*.=notice;*.=warn;\
		auth,authpriv.none;\
		cron,daemon.none;\
		mail,news,syslog.none		:omelasticsearch:
</pre>
 <p>
  The default template does a good job of transforming the arriving
  <i>
   rsyslog
  </i>
  messages (including timestamps, facility, priority,
sending hostname, etc) into JSON.
In our case, the application code sending messages on the "local0"
facility has already prepared messages in a rich JSON format already,
so we just use a basic
  <i>
   rsyslog
  </i>
  template that effectively passes
any "local0" messages straight through to the keeper.
 </p>
 <table border="0" cellpadding="0" cellspacing="0" width="100%">
  <tr>
   <td>
    <table border="0" cellpadding="0" cellspacing="15">
     <tr>
      <td>
       <p>
        For monitoring of the cluster, another PMDA exists which exports
  the statistics that
        <i>
         elasticsearch
        </i>
        makes available.
  This exports information about the individual cluster nodes (if more
  than one cluster node is in use) - their JVM memory statistics,
  cluster and node status, document indexing rates, etc.
       </p>
      </td>
     </tr>
     <tr>
      <td>
       <table border="0" cellpadding="10" cellspacing="20" width="100%">
        <tr>
         <td width="50%">
          <br/>
          Install
          <i>
           pmdaelasticsearch
          </i>
          :
          <pre><b># source /etc/pcp.env
# cd $PCP_PMDAS_DIR/elasticsearch
# ./Install </b></pre>
          <br/>
         </td>
        </tr>
       </table>
      </td>
     </tr>
     <tr>
      <td>
       <p>
        And the following metrics become available:
       </p>
       <pre><i>
elasticsearch.cluster.active_shards
elasticsearch.cluster.relocating_shards
elasticsearch.cluster.timed_out
elasticsearch.cluster.active_primary_shards
elasticsearch.cluster.number_of_nodes
elasticsearch.cluster.number_of_data_nodes
elasticsearch.cluster.cluster_name
elasticsearch.cluster.unassigned_shards
elasticsearch.cluster.initializing_shards
elasticsearch.cluster.status.code
elasticsearch.cluster.status.colour
</i></pre>
      </td>
     </tr>
    </table>
   </td>
   <td>
    <pre><i>
	elasticsearch.nodes.indices.size
	elasticsearch.nodes.cache.field_size
	elasticsearch.nodes.cache.field_evictions
	elasticsearch.nodes.cache.filter_size
	elasticsearch.nodes.cache.filter_evictions
	elasticsearch.nodes.cache.filter_count
	elasticsearch.nodes.merges.total_time
	elasticsearch.nodes.merges.current
	elasticsearch.nodes.merges.total
	elasticsearch.nodes.jvm.vm_version
	elasticsearch.nodes.jvm.version
	elasticsearch.nodes.jvm.vm_name
	elasticsearch.nodes.jvm.pid
	elasticsearch.nodes.jvm.uptime_s
	elasticsearch.nodes.jvm.uptime
	elasticsearch.nodes.jvm.gc.count
	elasticsearch.nodes.jvm.gc.time
	elasticsearch.nodes.jvm.gc.collectors.Copy.time
	elasticsearch.nodes.jvm.gc.collectors.Copy.count
	elasticsearch.nodes.jvm.gc.collectors.ParNew.count
	elasticsearch.nodes.jvm.gc.collectors.ParNew.time
	elasticsearch.nodes.jvm.gc.collectors.CMS.count
	elasticsearch.nodes.jvm.gc.collectors.CMS.time
	elasticsearch.nodes.jvm.threads.count
	elasticsearch.nodes.jvm.threads.peak_count
	elasticsearch.nodes.jvm.mem.heap_max
	elasticsearch.nodes.jvm.mem.heap_committed
	elasticsearch.nodes.jvm.mem.non_heap_init
	elasticsearch.nodes.jvm.mem.heap_init
	elasticsearch.nodes.jvm.mem.non_heap_committed
	elasticsearch.nodes.jvm.mem.non_heap_used
	elasticsearch.nodes.jvm.mem.non_heap_max
	elasticsearch.nodes.jvm.mem.heap_used
	elasticsearch.nodes.docs.count
	elasticsearch.nodes.docs.num_docs
</i></pre>
   </td>
  </tr>
 </table>
 <p>
  <b>
   Search queries
  </b>
 </p>
 <p>
  Now that data is being indexed in
  <i>
   elasticsearch
  </i>
  , we can make
interactive queries using
  <i>
   elasticsearch-head
  </i>
  , which looks a bit
like this:
 </p>
 <p>
 </p>
 <center>
  <img align="MIDDLE" alt="" height="490" src="images/elasticsearch.png" width="1356"/>
 </center>
 <p>
  <br/>
 </p>
 
</div>
</div>
</div>
</div>
</div>
<footer class='global-footer is-typeset'>
<div class='row-parent'>
<div class='row'>
<section class='row__colspaced'>
<div class='colspan12-3 colspan8-2 colspan6-2 colspan2-2 as-grid with-gutter'>
<div class='col__module'>
<h4>Main Menu</h4>
<ul>
<li>
<a href='/features.html'>Features</a>
</li>
<li>
<a href='/documentation.html'>Documentation</a>
</li>
<li>
<a class='download' href='/download.html'>Download</a>
</li>
</ul>
</div>
</div>
<div class='colspan12-3 colspan8-2 colspan6-2 colspan2-2 as-grid with-gutter'>
<div class='col__module'>
<h4>Developers</h4>
<ul>
<li>
<a href='https://github.com/performancecopilot/pcp/issues'>Report an issue</a>
</li>
<li>
<a href='https://github.com/performancecopilot/pcp/projects/1'>Roadmap</a>
</li>
<li>
<a href='/community.html'>Contributing</a>
</li>
<li>
<a href='/team.html'>Team</a>
</li>
</ul>
</div>
</div>
<div class='colspan12-3 colspan8-2 colspan6-2 colspan2-2 as-grid with-gutter'>
<div class='col__module'>
<h4>About</h4>
<ul>
<li>
<a href='/testimonials.html'>Testimonials</a>
</li>
<li>
<a href='/faq.html'>FAQ</a>
</li>
<li>
<a href='/website.html'>Website</a>
</li>
</ul>
</div>
</div>
<div class='colspan12-3 colspan8-2 colspan6-2 colspan2-2 as-grid with-gutter'>
<div class='col__module'>
<h4>Get Social</h4>
<ul>
<li>
<a class='twitter' href='https://twitter.com/performancepcp' rel='external'>Twitter</a>
</li>
<li>
<a class='github' href='https://github.com/performancecopilot' rel='external'>Github</a>
</li>
</ul>
</div>
</div>
</section>
</div>
<div class='row'>
<div class='colspan2-2'>
<p class='legal'>
This website is Copyright &copy; 2000-2004 Silicon Graphics Inc, 2007-2010 Aconex, 2012-2020 Red Hat
</p>
</div>
</div>
</div>
</footer>

</body>
</html>
